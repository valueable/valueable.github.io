<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>csc和cged总结</title>
      <link href="/2021/08/06/csc%E5%92%8Ccged%E6%80%BB%E7%BB%93/"/>
      <url>/2021/08/06/csc%E5%92%8Ccged%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="csc和cged总结"><a href="#csc和cged总结" class="headerlink" title="csc和cged总结"></a>csc和cged总结</h1><h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><blockquote><p>暑假对接一个公文智能核稿的项目，涉及到中文文本的纠错，大体包括中文拼写纠错（csc）、中文语法纠错（cged）以及其他业务相关的细化的错误纠正。<br>虽然毕设时做的是英语语法纠错，对纠错的实现的基本思路有个大概的把握，但是经过调研后发现中文拼写纠错较之英文有很大不同，如字音字形相近错误。<br>因此在项目中，参考了<a href="https://arxiv.org/abs/2004.14166">spellgcn</a>的思路用bert+gcn来实现中文拼写纠错；参考bert-ner框架以序列标注的思想来解决中文语法纠错问题（多词、少词、语序问题）</p></blockquote><span id="more"></span><h3 id="中文拼写纠错"><a href="#中文拼写纠错" class="headerlink" title="中文拼写纠错"></a>中文拼写纠错</h3><blockquote><p>中文拼写检查需要依赖中文字形字音的知识来解决形似音近出现的错误，因此spellgcn将语音和形状相似性整合到语义空间中，其模型结构如下：  </p></blockquote><p><img src="/images/csc.png" alt="png"> </p><blockquote><p>顺着这个思路，我决定做一个bert+gcn来解决中文拼写问题。</p></blockquote><p><strong>gcn</strong></p><blockquote><p>首先构建两个相似图（邻接矩阵）：语音相似图和字形相似图。邻接矩阵需要标准化（normalize），代码如下：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">A, symmetric=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="comment"># A = A+I</span></span><br><span class="line">    A = A + torch.eye(A.size(<span class="number">0</span>))</span><br><span class="line">    <span class="comment"># 所有节点的度</span></span><br><span class="line">    d = A.<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> symmetric:</span><br><span class="line">        <span class="comment"># D = D^-1/2</span></span><br><span class="line">        D = torch.diag(torch.<span class="built_in">pow</span>(d + <span class="number">1e-8</span>, -<span class="number">0.5</span>))</span><br><span class="line">        <span class="keyword">return</span> D.mm(A).mm(D)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># D=D^-1</span></span><br><span class="line">        D = torch.diag(torch.<span class="built_in">pow</span>(d + <span class="number">1e-8</span>, -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> D.mm(A)</span><br></pre></td></tr></table></figure><blockquote><p>对应公式如下：<br>$$ \hat{A} = D^{-\frac{1}{2}}(A + I)D^{-\frac{1}{2}} \tag1$$<br>图卷积公式如下<br>$$ f(A, H^l) = \hat{A}H^lW_g^l  \tag2$$<br>其中W是要学习的矩阵，A是混淆集矩阵，两个字相似为1，否则为0。l代表卷积的层数。<br>为了保持提取器的原始语义，引入attention（为字形、字音、字符三个特征赋予权重）<br>$$C_l^i = \sum_{k ∈{s, p}}a_{(i,k)}^lf_k(A^k,H^l)_i \tag3$$  </p></blockquote><blockquote><p>并且累积先前层的所有输出作为输出<br>$$H^{l+1} = C^l + \sum_{(i=0)}^lH^i \tag4$$<br><strong>项目工作流程</strong></p></blockquote><ul><li>获取bert的embedding矩阵</li><li>筛出作为图节点的word，先通过上一步的embedding做词嵌入，然后进入两层gcn得到node embedding</li><li>对于在混淆集的节点，使用node embedding；否则使用bert获取embedding。最终获取gcn_embedding。</li><li>用bert提取字符级特征，获取last hidden layer的输出。将此输出与gcn_embedding相乘。维度变化如下：<br>bert: (batch_size, sequence_length, hidden_size, 1)<br>gcn_embedding: (vocab_size, hidden_size)</li></ul><blockquote><p>由于pytorch的广播机制，最终经squeeze的维度为(batch_size, sequence_length, vocab_size)。<br>最后将得到的logit取max在词表中得到对应词，即完成中文拼写纠错的过程。</p></blockquote><p><strong>总结</strong></p><blockquote><p>spellgcn以图的形式引入了字音、字形特征，并将其与传统的bert获取的字符级嵌入做融合，再做词表级别的分类。直观理解，把混淆集做成GCN，每个字不仅是它自己融合上下文特征后的embedding，在embedding里也加入了音近、形近字的影子。于是进行多分类的时候，每个混淆集里的字都更倾向于分为混淆集里的其他字。<br>可以见得，spellgcn的成功在于分析领域任务的特点，在最终输出特征上面做文章。在模型训练和调优过程中，也逐渐意识到，对于中文拼写检查，数据是十分关键的，从spellgcn单用bert基线模型的效果非常好、使用gcn锦上添花就可以看出。因此，数据增强也是不得忽视的部分。</p></blockquote><h3 id="中文语法纠错"><a href="#中文语法纠错" class="headerlink" title="中文语法纠错"></a>中文语法纠错</h3><blockquote><p>根据需求，需要解决的是文章中的多词、少词、顺序问题。通过查阅已有的做法，决定将问题转化为一个序列标注问题：为每个词预测一个标签，对应上述错误中的一类，及对应ner中的B、I、O。<br>模型为Electra+crf。前向传播步骤如下:</p></blockquote><ul><li>先用electra根据输入获取高维语义特征last hidden layer和低维句法特征first hidden layer，将其融合最后再与last hidden layer拼接。</li><li>为防止过拟合，加入dropout并经过一个线性分类器得到logits。</li><li>如果是训练阶段送入crf计算loss，否则将logits送入crf做decoder，利用viterbi算法获取最有可能的tag。</li><li>crf的强大之处在于可以通过学习转移矩阵看前后的输出来判断当前输出。</li></ul><p><strong>总结</strong> </p><blockquote><p>相比于英语语法纠错，中文语法纠错的部分做的十分简单，一是因为需求所致，二是个人认为中文的纠错部分中心在csc。不过通过cged部分的学习，还是加深了对序列标注的理解，连带bert-ner的流程也熟练了。</p></blockquote><h3 id="量词名词搭配纠错"><a href="#量词名词搭配纠错" class="headerlink" title="量词名词搭配纠错"></a>量词名词搭配纠错</h3><blockquote><p>主要是用了我之前的一篇<a href="https://leolzh.top/2021/07/30/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0/">文章</a>所介绍的方法。<br>考虑到量词名词是一种搭配关系，引入依存句法分析和词性标注双重召回句子中量词-名词搭配。再根据已有词表做错误判断并选出候选集。</p></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote><p>以上三部分的纠错都是以深度学习的方式解决的，通过这个项目，涉猎了很多新知识，包括csc，cged，dep parse，gcn，ner等，可谓受益匪浅。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 工作总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 中文拼写和语法纠错 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读:ChineseBERT</title>
      <link href="/2021/08/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ChineseBERT/"/>
      <url>/2021/08/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ChineseBERT/</url>
      
        <content type="html"><![CDATA[<h1 id="论文阅读-ACL2021-ChineseBERT：融合字形与拼音信息的中文预训练模型"><a href="#论文阅读-ACL2021-ChineseBERT：融合字形与拼音信息的中文预训练模型" class="headerlink" title="论文阅读 ACL2021| ChineseBERT：融合字形与拼音信息的中文预训练模型"></a>论文阅读 ACL2021| ChineseBERT：融合字形与拼音信息的中文预训练模型</h1><h2 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h2><blockquote><p>这篇论文来自ACL2021，论文全名为ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information，即融合字形与拼音信息的中文预训练模型。  </p></blockquote><span id="more"></span><p>论文地址：<a href="https://link.zhihu.com/?target=https://aclanthology.org/2021.acl-long.161/">Paper</a><br>项目地址：<a href="https://link.zhihu.com/?target=https://github.com/ShannonAI/ChineseBert">Github</a></p><blockquote><p>目前，预训练语言模型在自然语言处理领域取得了显著地效果。但是由于BERT等预训练模式最初为英文设计，对于中文来说，<strong>汉语是一种符号语言，字符的符号包含一些额外的语义信息</strong>，因此，原始的预训练语言模型的形式缺失了字形信息和拼音信息两个重要的信息。<br>字形背后蕴含着丰富的语义，<strong>可以增强汉语自然语言模型的表现力</strong>。例如，“液”、“河”和“湖”都有“氵”，表示这些字符都与“水”的语义相关。<br>拼音，一个汉字的罗马化序列表示其发音，在建模语义和语法信息是至关重要的，同样的汉字在不同的读音下，有着不同的涵义。例如： “乐”字，读“yuè”时表示音乐，读“lè” 时表示快乐。<br>该论文将字形和拼音信息融入到预训练语言模型中，在中文多个领域（机器阅读理解、自然语言推理、文本分类、句对匹配、命名实体识别和分词任务上）都达到了SOTA。<br>下面主要针对ChineseBERT模型的模型结构、预训练设置以及结果分析进行详细介绍，配以我个人对其源码的一些薄见。</p></blockquote><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><blockquote><p>模型整体结构如下，模型将字符嵌入（char embedding）、字形嵌入（glyph embedding）和拼音嵌入（pinyin embedding）进行拼接起来；然后通过融合层，得到一个d维融合嵌入（fusion embedding）；最后将其与位置嵌入（position embedding）、片段嵌入（segment embedding）相加，形成Transformer-Encoder层的输入。<br>由于预训练时没有使用NSP任务，因此模型结构图省略了片段嵌入（segment embedding）。实际上下游任务输入为多个段落时（例如：文本匹配、阅读理解等任务），是采用了segment embedding，详细见：fusion_embedding.py文件的34行。<br><img src="/images/ChineseBERT.png" alt="png"></p></blockquote><h3 id="Char-embedding"><a href="#Char-embedding" class="headerlink" title="Char embedding"></a>Char embedding</h3><blockquote><p>字符嵌入与原始BERT模型中的token embedding一致，仅在字符粒度上进行embedding。</p></blockquote><h3 id="Glyph-embedding"><a href="#Glyph-embedding" class="headerlink" title="Glyph embedding"></a>Glyph embedding</h3><blockquote><p>字形嵌入采用三种中文字体（仿宋、行楷和隶书）。先将每个汉字用以24x24的向量表示，然后做flattern操作，最后将该汉字的三种字体的flattern结果做拼接，这样获得的特征维度为24x24x3=1728，在经过一个linear将特征放缩到hidden_state的维度。<br><img src="/images/ChineseBERT-1.png" alt="png"></p></blockquote><h3 id="Pinyi-embedding"><a href="#Pinyi-embedding" class="headerlink" title="Pinyi embedding"></a>Pinyi embedding</h3><blockquote><p>拼音嵌入首先使用pypinyin将每个汉字的拼音转化为罗马化字的字符序列，其中也包含了音调。比如对汉字“猫”，其拼音字符序列就是“mao1”。对于多音字如“乐”，pypinyin能够非常准确地识别当前上下文中正确的拼音，因此ChineseBERT直接使用pypinyin给出的结果。<br>处理过程如下：</p></blockquote><ul><li>embedding -&gt; [bs,sentence_length,pinyin_locs,embed_size]</li><li>view, permute, -&gt; [(bs*sentence_length), embed_size, pinyin_locs]</li><li>conv1d -&gt; [(bs*sentence_length), pinyin_out_dim, H=pinyin_locs-1]</li><li>max_pool1d -&gt; [(bs*sentence_length),pinyin_out_dim,1]</li><li>view [bs,sentence_length,pinyin_out_dim]  </li></ul><blockquote><p><img src="/images/ChineseBERT-2.png" alt="png"></p></blockquote><h3 id="Fusion-embedding"><a href="#Fusion-embedding" class="headerlink" title="Fusion embedding"></a>Fusion embedding</h3><blockquote><p>将汉字的字嵌入、字形嵌入与拼音嵌入拼接在一起，然后经过一个全连接层，就得到了该汉字对应的融合嵌入。每个汉字对应的融合嵌入与位置嵌入相加，再加之layernorm，dropout防止过拟合，就是每个汉字给模型的输入。模型的输出就是每个汉字对应的高维向量表征，基于该向量表征对模型进行预训练。<br><img src="/images/ChineseBERT-3.png" alt="png"></p></blockquote><h2 id="Pretraining-setup"><a href="#Pretraining-setup" class="headerlink" title="Pretraining setup"></a>Pretraining setup</h2><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><blockquote><p>数据来自<a href="https://commoncrawl.org/">CommonCrawl</a>。在经过数据清洗后，用于预训练ChineseBERT的数据规模为约4B个汉字。并在全词掩码时使用LTP工具进行词语识别。</p></blockquote><h3 id="Masking-strategies"><a href="#Masking-strategies" class="headerlink" title="Masking strategies"></a>Masking strategies</h3><blockquote><p>在ChineseBERT模型中，使用全词掩码（Whole Word Masking，WWM）和字符掩码（Char Masking，CM）两种策略。<br>字符掩码，即对单独的字符进行MASK；全词掩码，即对一个词语中所有的字符进行MASK。</p></blockquote><h3 id="Pretraining-details"><a href="#Pretraining-details" class="headerlink" title="Pretraining details"></a>Pretraining details</h3><blockquote><p>ChineseBERT模型与原始BERT模型结构不同，因此，没有加载原始模型的参数，而是从头进行预训练的。为了解决长短依赖的问题，在训练过程中，采用packed input和single input交替训练，<strong>比例为9:1</strong>，其中single input为一个单句，packed input由总长度不超过512字符的多个单句拼接而成。并且90%的概率进行全字掩码，10%的概率进行字符掩码。词语或字符的mask概率为15%，80%的概率将mask的字符使用[MASK]替换，10%的概率将mask的字符使用随机字符替换，10%的概率将mask的字符保持不变。采用了动态掩码策略来避免数据的重复训练。<br><strong>Base与Large模型的层数与原始BERT一致，分别是12层和24层，输入维度和头数分别为768/12和1024/16。Base模型训练了500K步，学习率为1e-4，warmup步数为20k，batch大小为3.2K。Base模型训练280K步，学习率为3e-4，warmup步数为90k，batch大小为8K</strong>。</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>详见论文对应章节</p><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><blockquote><p>Github中给了详细的代码，可以直接调用，具体如下：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets.bert_dataset <span class="keyword">import</span> BertDataset</span><br><span class="line"><span class="keyword">from</span> models.modeling_glycebert <span class="keyword">import</span> GlyceBertModel</span><br><span class="line">bert_path = <span class="string">&quot;&quot;</span></span><br><span class="line">tokenizer = BertDataset(bert_path)</span><br><span class="line">chinese_bert = GlyceBertModel.from_pretrained(bert_path )</span><br><span class="line">sentence = <span class="string">&#x27;我喜欢猫&#x27;</span></span><br><span class="line">input_ids, pinyin_ids = tokenizer.tokenize_sentence(sentence)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input_ids:&quot;</span>, input_ids)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;pinyin_ids:&quot;</span>, pinyin_ids)</span><br><span class="line"></span><br><span class="line">length = input_ids.shape[<span class="number">0</span>]</span><br><span class="line">input_ids = input_ids.view(<span class="number">1</span>, length)</span><br><span class="line">pinyin_ids = pinyin_ids.view(<span class="number">1</span>, length, <span class="number">8</span>)</span><br><span class="line">output_hidden = chinese_bert.forward(input_ids, pinyin_ids)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;output_hidden:&quot;</span>, output_hidden)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><blockquote><p>在预训练时候，该论文丢弃了SOP或NSP任务，有一些疑惑，在实验中也没有提到；并且模型是从头训练也比较奇怪，虽然说embedding层不一样，但是transformer-encoder层的参数还是可以用的。<br>​增加字形和拼音特征的思路还是比较正的，并且评测数据均为中文数据。也正是因为这部分，我觉得像中文拼写纠错里面的SpellGCN所做的工作，这才引起了对本论文的兴趣。<br>总之，在后续做模型融合时，多了一个可用模型。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 日常学习 </tag>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对比学习</title>
      <link href="/2021/08/01/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/08/01/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h1><h3 id="学习初衷"><a href="#学习初衷" class="headerlink" title="学习初衷"></a>学习初衷</h3><blockquote><p>最近看到好多推送对比学习的公众号，包括在和师兄做AAAI时也用到了对比学习的思想，于是决定了解下对比学习。主要是为了对其有个认识，拓宽视野。</p></blockquote><span id="more"></span><h3 id="对比学习（Contrastive-Learning）：研究进展精要"><a href="#对比学习（Contrastive-Learning）：研究进展精要" class="headerlink" title="对比学习（Contrastive Learning）：研究进展精要"></a>对比学习（Contrastive Learning）：研究进展精要</h3><blockquote><p>内容<strong>源自<a href="https://zhuanlan.zhihu.com/p/367290573">知乎同名文章-对比学习（Contrastive Learning）：研究进展精要</a>一文</strong>。<br>Bert预训练模型，通过MLM人物的自监督学习，充分挖掘了从海量无标注文本中学习通用知识的能力，而图像领域的预训练，往往是有监督的，就是用ImageNet来进行预训练，但是在下游任务中Fine-tuning的效果，跟Bert在NLP下游任务中带来的性能提升是没法比的。<br>有监督训练的典型问题就是标注数据有限。NLP领域目前的经验是，自监督预训练使用的数据量越大，模型越复杂，那么模型能够吸收的知识越多，对下游任务来说越好。图像领域入伙技术要有质的提升，可能也必须走这条路，即：充分使用越来越大量的无标注数据，使用越来越复杂的模型，采用自监督预训练模式来从中吸取图像本身的先验知识分布，在下游任务中通过Fine-tuning，来把预训练过程学习到的知识，迁移给并提升下游任务的效果。<br>对比学习是自监督学习的一种，也就是说，不依赖标注数据，要从无标注图像中自己学习知识。我们知道，自监督学习其实在图像领域里已经被探索了很久了。总体而言，图像领域里的自监督可以分为两种类型：生成式自监督学习，判别式自监督学习。VAE和GAN是生成式自监督学习的两类典型方法，即它要求模型重建图像或者图像的一部分，这类型的任务难度相对比较高，要求像素级的重构，中间的图像编码必须包含很多细节信息。对比学习则是典型的判别式自监督学习，相对生成式自监督学习，对比学习的任务难度要低一些。目前，对比学习貌似处于“无明确定义、有指导原则”的状态，它的指导原则是：通过自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远。而如何构造相似实例，以及不相似实例，如何构造能够遵循上述指导原则的表示学习模型结构，以及如何防止模型坍塌(Model Collapse)，这几个点是其中的关键。<br>目前出现的对比学习方法已有很多，如果从防止模型坍塌的不同方法角度，我们可大致把现有方法划分为：基于负例的对比学习方法、基于对比聚类的方法、基于不对称网络结构的方法，以及基于冗余消除损失函数的方法。</p></blockquote><h4 id="基于负例的对比学习：以SimCLR为例"><a href="#基于负例的对比学习：以SimCLR为例" class="headerlink" title="基于负例的对比学习：以SimCLR为例"></a>基于负例的对比学习：以SimCLR为例</h4><blockquote><p>之所以首先选择SimCLR来介绍，一方面是SimCLR的效果相对它提出之前的模型，效果好得比较明显；另外一方面SimCLR采取对称结构，整体相对简洁清晰，也比较容易说清楚。而且，它奠定的结构，已成为其它对比学习模型的标准构成部分，搞明白了SimCLR，再理解其它模型，相对而言会更容易一些。<br>对比学习是自监督学习，我们没有标注数据，所以需要自己构造相似数据（正例）以及不相似数据（负例）<br><img src="/images/contrastivelearning-pic1.png" alt="png"><br>如图，对于某张图片，从可能的增强操作集合$T$中，随机抽取两种：$t_1-T$及$t_2-T$，分别作用在原始图像上形成两张经过增强的新图像$&lt;x_1, x_2&gt;$,两者互为正例。训练时，batch内其他任意图像，都可作为$x_1$或$x_2$的负例。这样，对比学习希望习得某个表示模型，它能够将图片映射到某个投影空间，并在这个空间内拉近正例的距离，推远负例距离。也就是说，迫使表示模型能够忽略表面因素，学习图像的内在一致结构信息，即学会某些类型的不变性，比如遮挡不变性、旋转不变性、颜色不变性等。SimCLR证明了，如果能够同时融合多种图像增强操作，增加对比学习模型任务难度，对于对比学习效果有明显提升作用。<br>有了正例和负例，接下来需要做的是：构造一个表示学习系统，通过它将训练数据投影到某个表示空间内，并采取一定的方法，使得正例距离能够比较近，负例距离比较远。在这个对比学习的指导原则下，我们来看SimCLR是如何构造表示学习系统的。<br><img src="/images/contrastivelearning-pic2.png" alt="png"><br>我们随机从无标训练数据中取N个构成一个Batch，对于Batch里的任意图像，根据上述方法构造正例，形成两个图像增强视图：Aug1和Aug2。Aug1 和Aug2各自包含N个增强数据，并分别经过上下两个分枝，对增强图像做非线性变换，这两个分枝就是SimCLR设计出的表示学习所需的投影函数，负责将图像数据投影到某个表示空间。<br>因为上下分枝是对称的，所以我们仅以增强视图Aug1所经过的上分枝来介绍投影过程。Aug1首先经过特征编码器Encoder（一般采用ResNet做为模型结构，这里以函数 $f_θ(x)$ 代表），经CNN转换成对应的特征表示 $h_i$ 。紧随其后，是另外一个非线性变换结构Projector（由[FC-&gt;BN-&gt;ReLU-&gt;FC]两层MLP构成，这里以函数 $g_θ(·)$ 代表），进一步将特征表示 $h_i$ 映射成另外一个空间里的向量 $z_i$ 。这样，增强图像经过 $g_θ(f_θ(x))$ 两次非线性变换，就将增强图像投影到了表示空间，下分枝的Aug2过程类似。这会引发一个问题：为什么这种投影操作，要做两次非线性变换，而不是直接在Encoder后，只经过一次变换即可呢？这个问题的答案，稍后我们会给出解释。<br>对于Batch内某张图像 $x$ 来说，在Aug1和Aug2里的对应的增强后图像分别是 $x_i$ 和 $x_j$ ，那么数据对 $&lt;x_i, x_j&gt;$ 互为正例，而 $x_i$ 和Aug1及Aug2里除 $x_j$ 之外的其它任意2N-2个图像都互为负例。在经过 $g_θ(f_θ(x))$ 变换后，增强图像被投影到表示空间。在表示空间内，我们希望正例距离较近，负例距离较远。如果希望达成这一点，一般通过定义合适的损失函数来实现。在介绍损失函数前，我们首先需要一个度量函数，以判断两个向量在投影空间里的距离远近，一般采用相似性函数来作为距离度量标准。具体而言，相似性计算函数采取对表示向量L2正则后的点积或者表示向量间的Cosine相似性：</p></blockquote><p>$$S(z_i, z_j) = z_i^Tz_j/(||z_i||_2||z_j||_2) \tag1$$</p><blockquote><p>SimCLR的损失函数采用InfoNCE Loss。  </p></blockquote><p>$$L_{bt} = \sum_i(1-C_{ii})^2 + \lambda \sum_i\sum_{j \neq i}C_{ij}^2  \tag2$$</p><blockquote><p>其中$&lt;z_i, z_i^+&gt;$表示两个正例相应的表示向量。函数分子部分鼓励正例相似度越高越好，分母鼓励负例之间相似度越低越好。<br><strong>上面介绍了SimCLR的关键做法，本身这个过程，其实是标准的预训练模式；利用海量的无标注图像数据，根据对比学习指导原则，学习出好的Encoder模型以及它对应产生的特征表示。所谓好的Encoder，就是说输入图像，它能学会并抽取出关键特征，这个过程跟Bert模型通过MLM自监督预训练其实目的相同，只是做法有差异。学好Encoder后，可以在解决下游具体任务的时候，用学到的参数初始化Encoder中的ResNet模型，用下游任务标注数据来Fine-tuning模型参数，期待预训练阶段学到的知识对下游任务有迁移作用。由此可见，SimCLR看着有很多构件，比如Encoder、Projector、图像增强、InfoNCE损失函数，其实我们最后要的，只是Encoder，而其它所有构件以及损失函数，只是用于训练出高质量Encoder的辅助结构。目前所有对比学习模型都是如此，这点还请注意</strong><br>至于为什么使用encoder和projector做两次投影变换，则是出自经验结果，实验证明，加上projector效果提升明显。<br>SimCLR论文中，对于Projector和Encoder的编码差异进行了对比实验，结论是：Encoder后的特征表示，会有更多包含图像增强信息在内的细节特征，而这些细节信息经过Projector后，很多被过滤掉了。虽然为何需要两次非线性变换，目前只有实验结果，并未有理论解释。可能是如下原因：一般的特征抽取器，在做特征提取的时候，底层偏向抽取通用的低层特征，往往与任务无关，通用性强；接近比如分类任务的高层网络结构，更倾向编码任务相关的高阶特征信息。想来，Encoder和Projector也应该如此，也就是说，在接近任务的高层网络，也就是Projector，会编码更多跟对比学习任务相关的信息，低层就是Encoder，会编码更多跟任务无关的通用细节信息。对于下游任务，这种对比学习训练任务相关的特征，可能会带来负面影响。如果映射网络只包含Encoder的话，那么特征表示里会有很多预训练任务相关特征，会影响下游任务效果；而加上Projector，等于增加了网络层深，这些任务相关特征就聚集在Projector，此时Encoder则不再包含预训练任务相关特征，只包含更通用的细节特征。这是为何需要两次映射过程。以上仅为一种可能原因，并不保证正确性。在此之后的对比学习模型，基本都采取了Encoder+Projector的两次映射结构，以及复合图像增强方法。  </p></blockquote><h4 id="对比学习到底在干什么"><a href="#对比学习到底在干什么" class="headerlink" title="对比学习到底在干什么"></a>对比学习到底在干什么</h4><blockquote><p>对比学习在做特征表示相似性计算时，要先对表示向量做L2正则，之后再做点积计算，或者直接采用Cosine相似性，为什么要这么做呢？现在很多研究表明，把特征表示 $g_θ(f_\Theta(x))$ 映射到单位超球面上，有很多好处。这里有两个关键，一个是单位长度，一个是超球面。首先，相比带有向量长度信息的点积，在去掉长度信息后的单位长度向量 $g_θ(f_θ(x))/{||g_θ(f_θ(x))||}^2$ 上操作，能增加深度学习模型的训练稳定性。另外，当表示向量 $g_θ(f_θ(x))$ 被映射到超球面上，如果模型的表示能力足够好，能够把相似的例子（比如带有相同类标号的数据）在超球面上聚集到较近区域，那么很容易使用线性分类器把某类和其它类区分开（参考上图）。在对比学习模型里，对学习到的表示向量 $g_θ(f_θ(x))$ 进行L2正则，或者采用Cosine相似性，就等价于将表示向量 $g_θ(f_θ(x))$ 投影到了单位超球面上进行相互比较。很多对比学习模型相关实验也证明了：对表示向量进行L2正则能提升模型效果。这是为何一般要对表示向量进行L2正则操作的原因。<br>那么，好的对比学习系统，应该具备怎样的潜在能力呢？论文“Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere”对这个问题进行了探讨。它提出了好的对比学习系统应该具备两个属性：Alignment和Uniformity（参考上图）。所谓“Alignment”，指的是相似的例子，也就是正例，映射到单位超球面后，应该有接近的特征，也即是说，在超球面上距离比较近；所谓“Uniformity”，指的是系统应该倾向在特征里保留尽可能多的信息，这等价于使得映射到单位超球面的特征，尽可能均匀地分布在球面上，分布得越均匀，意味着保留的信息越充分。乍一看不好理解“分布均匀和保留信息”两者之间的关联，其实道理很简单：分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分。<br><img src="/images/contrastivelearning-pic4.png" alt="png"><br>回顾infoNCE Loss,分子部分鼓励正例之间越近越好，这体现了Alignment属性；分母部分鼓励负例直接距离越远越好，这体现了Uniformity属性，使得负例的特征更均匀的分布在球面。<br>损失函数中的温度（tempture）参数（一帮设置为0.1或0.2）$\tau$易被忽视，实际上作用很大（在AAAI中也用到了这个超参），温度参数会将模型更新的重点，聚焦到有难度的负例，并对它们做相应的惩罚，难度越大，也即是与$x_i$距离越近，则分配到的惩罚越多。所谓惩罚，就是在模型优化过程中，将这些负例从 $x_i$ 身边推开，是一种斥力。也就是说，距离 $x_i$ 越近的负例，温度超参会赋予更多的排斥力，将它从 $x_i$ 推远。而如果温度超参数 $\tau$ 设置得越小，则InfoNCE分配惩罚项的范围越窄，更聚焦在距离 $x_i$ 比较近的较小范围内的负例里。同时，这些被覆盖到的负例，因为数量减少了，所以，每个负例，会承担更大的斥力。但是$\tau$并非越小越好，太小的温度参数有可能会将正例也造成惩罚。</p></blockquote><h4 id="更多对比学习模型"><a href="#更多对比学习模型" class="headerlink" title="更多对比学习模型"></a>更多对比学习模型</h4><blockquote><p>可以参考文章的<strong>基于负例的对比学习：Batch之外的<a href="https://zhuanlan.zhihu.com/p/367290573">部分</a>。</strong></p></blockquote><h4 id="问题与方向：向何处去"><a href="#问题与方向：向何处去" class="headerlink" title="问题与方向：向何处去"></a>问题与方向：向何处去</h4><blockquote><p>我们知道，自监督方法的一个潜在好处是；理论上随着无标注数据训练数据量的增大，那么自监督模型学习到的图像天然结构知识越丰富，应用在下游业务效果也应该越好。但是，从这一点上来看，目前的对比学习是有局限的，实验表明，当自监督数据量增加到非常大的时候，貌似并未能带来更多明显收益与效果。这说明目前的对比学习也好，自监督学习也好，仍然有很大的改进空间。<br>训练数据偏置（Bias）问题<br>目前大多数对比学习采用的自监督训练数据，一般会用ImageNet，但是用ImageNet做为对比学习训练数据，有个问题： ImageNet这种精挑细选出来的用于分类的数据，往往一个图片仅包含单个类别的实例，这也好理解，本来就是用来分类的，如果一张图片包含多个不同类的实体，那分类标签没法打。但是这带来的问题是：这种数据还是太干净了，如果我们在网上随意挑选图片，大多数都在描述一个场景，情况比较复杂，可能一张图片包含多个不同类别的实例。而目前的对比学习方法，对于处理此类一张图包含多类别实体的图片，效果并不好，这就是所谓的训练数据偏置问题。我个人认为，这个问题还是比较严重的，因为要想真正做到自监督，对训练数据要求就要放低，这样才能快速扩充更多数据用来训练。CASTing Your Model:Learning to Localize Improves Self-Supervised Representations文章探讨了这个问题并提出了一些解决办法。</p></blockquote><blockquote><p>更好构建正例的方法<br>正确地做图像增强，构建好的有难度的正例，对于对比学习是十分关键的。这样我们可以让表示学习系统，学到更多种类的图像不变性，增强表示学习模型的表达能力。虽然目前有很多图像增强方法，但是目前研究（参考：Demystifying Contrastive Self-Supervised Learning:Invariances, Augmentations and Dataset Biases）表明，与监督学习相比，对比学习模型主要学习到的，更多是一种图像遮挡不变性和颜色不变性，对于其它的常见不变形，比如视角不变性、照明不变性等，对比学习模型的效果要明显弱于监督学习。所以，如何构造正例，使得对比学习模型能学习更多种类的空间不变性，也是非常关键的。</p></blockquote><blockquote><p>像素级学习能力<br>从目前对比学习模型的运行机制看，因为是判别模型，目标相对容易，表示学习系统能学到细粒度的特征信息，但是对于像素级特征，应该是表达能力不足的。而图像处理具体的子领域众多，除了常见的分类任务，很多任务比如Object Segmentation等，都需要在像素级进行操作，所以如何改造现有对比学习模型，使得它能够更好地帮助像素级下游任务，也是比较重要的。论文Dense Contrastive Learning for Self-Supervised Visual Pre-Training针对这个问题，提出了改进方法。</p></blockquote><blockquote><p>除了上述几个问题，其实还有很多可以列在这里的，比如只使用正例的对比学习模型，从原理上讲，到底为何模型能够不坍塌？再比如，目前很多探索，集中研究对比学习中的Hard负例问题。我们从上文讲解可知：对于负例对比学习方法，之所以负例越多模型效果越好，其实本质上，是因为越多负例，会包含更多的Hard负例，而这些Hard负例对于模型贡献较大，而easy负例，其实没多少贡献。但是，我们又知道，温度超参本身其实是可以聚焦在Hard负例上的。那么，Hard负例应该研究什么具体问题？这个需要仔细考虑。再比如，目前不少研究在考虑融合有监督模型和对比学习，试图兼具两者的优点，这个有多大意义？……..诸如此类，很多问题与方向，不一而足，篇幅原因，到此为止。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日常学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>依存句法分析学习</title>
      <link href="/2021/07/30/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/07/30/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="依存句法分析学习"><a href="#依存句法分析学习" class="headerlink" title="依存句法分析学习"></a>依存句法分析学习</h1><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><blockquote><p>由于项目需要，得分析如何解决中文句子里面量词和名词的搭配问题。之前考虑使用CSC、CGED任务中比较完善的模型去解。项目中已经用到CSC中效果比较好的BERT+GCN去解决，<strong>受到来自<a href="https://github.com/ACL2020SpellGCN/SpellGCN">SpellGCN</a>的启发</strong>，使用GCN替代传统的MLP，从而对字音字形相似具有更好的处理。但发现对这种指定的搭配错误没什么效果，于是准备先用依存句法分析去解决，如果效果也不如预期，在考虑针对此类任务扩充训练数据。</p></blockquote><span id="more"></span><h3 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h3><blockquote><p>考虑到要解决量词和名词的搭配问题，决定先从词性标注的角度出发，把一组量词-名词对找出来，但这样的问题在于量词名词间可能有很多修饰词，另一方面可能量词在名词后，如句子：“参与者可获得汽车一台。”<br>自己用规则写了半天，自以为很完备，但总有想不到的情况。</p></blockquote><blockquote><p>在师兄提醒下尝试使用依存句法分析去解决，个人使用了LTP、HanLP等工具，都有些不尽人意。最后考虑了<strong>百度开源的<a href="https://github.com/baidu/DDParser">DDParser</a>项目</strong>。基本流程如下：</p></blockquote><ul><li>先用百度的LAC分词工具进行分词，这里也需要用到分词后的词性。</li><li>封装分词后的结果作为dataset并以此构建dataloader</li><li>feed入ernie-lstm模型并获取各个词的head（关系对象）以及关系。  </li></ul><blockquote><p>DDParser项目参考的是2017年的<strong>ICLR<a href="https://arxiv.org/abs/1611.01734">《Deep Biaffine Attention for Neural Dependency Parsing》</a>一文</strong>，于是顺藤摸瓜，看看其依存句法分析的思路。  </p></blockquote><h3 id="论文解读"><a href="#论文解读" class="headerlink" title="论文解读"></a>论文解读</h3><blockquote><p>这篇论文是斯坦福专攻Dependency Parsing的博士生Dozat在ICLR 2017上的论文，拿到了graph-based方法中的最高分，改进版还拿到了CoNLL 2017 Shared Task的第一。基于图的依存句法分析需要解决两个问题：1、哪两个节点连依存弧；2、弧的标签是什么。NN方法使用分类器来解决这两个问题，记feature detector得到的特征向量为$r_i$。  </p></blockquote><h4 id="arc得分"><a href="#arc得分" class="headerlink" title="arc得分"></a>arc得分</h4><blockquote><p>这是一个不定类别的多分类问题。若句子中有n个单词，包含虚根ROOT在内一共d=n+1个token。对每个token来讲，都需要得到一个分数向量$s_i∈R^{d×1}$。所有token构成$R^{d×d}$的分数矩阵，最后根据arc-factored原理找MST(最小生成树)。<br>一般的MLP是固定类别的分类器，无法解决不定类别的分类问题。作者提出先将特征向量$r_i$做一个线性变换降维为k，即$R^{k×1}$。<br>$$h_i^{(arc_dep)} = MLP^{(arc_dep)}(r_i) \tag1$$<br>$$h_j^{(arc_head)} = MLP^{(arc_head)}(r_j) \tag2$$</p></blockquote><blockquote><p>其中MLP为dep和head专用，k通常维度更小，这样压缩维度的好处是可以去除一些冗余信息。<br>特征降维了之后，权值矩阵和偏置也必须做出调整。作者提出用两个矩阵连乘（两次仿射变换biaffine）输入向量:<br>$$s_i^{(arc)} = {H^{(arc_head)}U^{(1)}h_i^{(arc-dep)} + H^{(arc_head)}u^{(2)}} \tag3$$</p></blockquote><blockquote><p>其中矩阵H是d个token的特征经过MLP后的stack形式，上式维度变化为：<br>$$(d×k)(k×k)(k×1)+(d×k)(k×1)=(d×1) $$</p></blockquote><blockquote><p>结果是拿到了d个token的分数$R^{d×1}$，同时分类器又不需要维护多个不同大小的权值矩阵（只需一个$R^{k×k}$的矩阵和两个MLP），漂亮地实现了可变类别分类。<br>如果把偏置放到$U^{(1)}$里面去，同时进行d个token的运算，则上式的维度变化是:<br>$$(d×(k+1))((k+1)×k)(k×d)=(d×d)$$</p></blockquote><blockquote><p>作者称这3个式子为deep bilinear attention mechanism，因为不同于直接用RNN输出的feature （shallow），这里通过MLP二次encode了一下。叫attention的原因是，输入是所有时刻的RNN feature，输出是一个在各个时刻上归一化的向量。<br>整个网络架构如下:</p></blockquote><p><img src="/images/DEPModel.png" alt="png"> </p><blockquote><p>从下往上看，输入是word以及pos的embedding拼接，BiLSTM提取到特征$r_i$，经过两个不同的MLP分别得到$h^{(arc−dep)}_i$ 和 $h^{(arc−head)}_j$ ，d个这样的h stack起来分别构成了$H^{(arc−dep)}$和$H^{(arc−head)}$。其中由于bias的原因，$H^{(arc−dep)}$额外拼接了一个单位向量。经过一个中间矩阵$U^{(arc)}$仿射变换后，每个token以dep的身份与以head的身份的每个token进行一次点积，得到arc成立的分数矩阵$S^{(arc)}$。</p></blockquote><h4 id="arc标签"><a href="#arc标签" class="headerlink" title="arc标签"></a>arc标签</h4><blockquote><p>这是个固定类别的分类问题，作者认为必须同时考虑依存关系的先验概率，已知词语作为head或dep接受某种依存关系的后验概率。也就是下式:<br>$$S_i^{(label)}= {r^{T}_{(y_i)}U^{(1)}}+ (ry_i ⊕ r_i)^{T}U^{(2)} + b \tag4$$<br>第一项是同时已知i作为dep、$y_i$作为head情况下的后验概率，第二项是已知i或$y_i$是arc两端的后验概率，第三项偏置是什么都不知道时label的先验概率。<br>关于第二项的⊕的解读。拼接的话向量是有先后顺序的，应该用没有顺序的求和。这里看看两个向量（书写简便，就用仅含有一个元素的“向量”做例子）x1和x2的拼接乘上一个矩阵:</p></blockquote><p>$$\left(\begin{array}{c}a&amp;d\\<br>b&amp;e\\<br>c&amp;f\end{array} \right)<br>\left(\begin{array}{c}x_1\\<br>x_2 \end{array}\right) = \left(\begin{array}{c}ax_1+dx_2\\<br> bx_1+ex_2 \\<br> cx_1+fx_2\end{array}\right)$$</p><blockquote><p>可以看出，拼接后的向量与一个向量相乘的结果，是与拼接顺序无关的。第一项中含有一个高阶tensor，假设一共有m种label，上式第一项的维度变化为：<br>$$(1×k)(m×k×k)(k×1)=(m×1×1)$$</p></blockquote><h3 id="源码浅析"><a href="#源码浅析" class="headerlink" title="源码浅析"></a>源码浅析</h3><blockquote><p>项目源码的模型为ernie-lstm，使用ernie获取embedding，然后再用上述方法分别得到arc和relationship。</p></blockquote><h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><blockquote><p>利用DDParser，取出具有定中关系的、且包含量词（通过返回的词性判断量词）的词对。以名词为key在已有词典中做匹配，如果当前量词不在key对应的value list中，则为量词-名词搭配错误，并给出改正建议。</p></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote><p>方法较之我之前写的规则要简单，效果也有保证。在阅读论文过程中，作者用的几个仿射变换十分巧妙，我阅读时从要得到的结果反推中间的仿射会比较好理解，相信作者的思路也与之相似。最后就是DDParser介入了预训练模型（ERNIE），pretrained model的引入已经是一种趋势了。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 依存句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 日常学习 </tag>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
