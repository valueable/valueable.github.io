<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>对比学习</title>
      <link href="/2021/08/01/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/08/01/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h1><h3 id="学习初衷"><a href="#学习初衷" class="headerlink" title="学习初衷"></a>学习初衷</h3><blockquote><p>最近看到好多推送对比学习的公众号，包括在和师兄做AAAI时也用到了对比学习的思想，于是决定了解下对比学习。主要是为了对其有个认识，拓宽视野。</p></blockquote><h3 id="对比学习（Contrastive-Learning）：研究进展精要"><a href="#对比学习（Contrastive-Learning）：研究进展精要" class="headerlink" title="对比学习（Contrastive Learning）：研究进展精要"></a>对比学习（Contrastive Learning）：研究进展精要</h3><blockquote><p>内容<strong>源自<a href="https://zhuanlan.zhihu.com/p/367290573">知乎同名文章-对比学习（Contrastive Learning）：研究进展精要</a>一文</strong>。<br>Bert预训练模型，通过MLM人物的自监督学习，充分挖掘了从海量无标注文本中学习通用知识的能力，而图像领域的预训练，往往是有监督的，就是用ImageNet来进行预训练，但是在下游任务中Fine-tuning的效果，跟Bert在NLP下游任务中带来的性能提升是没法比的。<br>有监督训练的典型问题就是标注数据有限。NLP领域目前的经验是，自监督预训练使用的数据量越大，模型越复杂，那么模型能够吸收的知识越多，对下游任务来说越好。图像领域入伙技术要有质的提升，可能也必须走这条路，即：充分使用越来越大量的无标注数据，使用越来越复杂的模型，采用自监督预训练模式来从中吸取图像本身的先验知识分布，在下游任务中通过Fine-tuning，来把预训练过程学习到的知识，迁移给并提升下游任务的效果。<br>对比学习是自监督学习的一种，也就是说，不依赖标注数据，要从无标注图像中自己学习知识。我们知道，自监督学习其实在图像领域里已经被探索了很久了。总体而言，图像领域里的自监督可以分为两种类型：生成式自监督学习，判别式自监督学习。VAE和GAN是生成式自监督学习的两类典型方法，即它要求模型重建图像或者图像的一部分，这类型的任务难度相对比较高，要求像素级的重构，中间的图像编码必须包含很多细节信息。对比学习则是典型的判别式自监督学习，相对生成式自监督学习，对比学习的任务难度要低一些。目前，对比学习貌似处于“无明确定义、有指导原则”的状态，它的指导原则是：通过自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远。而如何构造相似实例，以及不相似实例，如何构造能够遵循上述指导原则的表示学习模型结构，以及如何防止模型坍塌(Model Collapse)，这几个点是其中的关键。<br>目前出现的对比学习方法已有很多，如果从防止模型坍塌的不同方法角度，我们可大致把现有方法划分为：基于负例的对比学习方法、基于对比聚类的方法、基于不对称网络结构的方法，以及基于冗余消除损失函数的方法。</p></blockquote><h4 id="基于负例的对比学习：以SimCLR为例"><a href="#基于负例的对比学习：以SimCLR为例" class="headerlink" title="基于负例的对比学习：以SimCLR为例"></a>基于负例的对比学习：以SimCLR为例</h4><blockquote><p>之所以首先选择SimCLR来介绍，一方面是SimCLR的效果相对它提出之前的模型，效果好得比较明显；另外一方面SimCLR采取对称结构，整体相对简洁清晰，也比较容易说清楚。而且，它奠定的结构，已成为其它对比学习模型的标准构成部分，搞明白了SimCLR，再理解其它模型，相对而言会更容易一些。<br>对比学习是自监督学习，我们没有标注数据，所以需要自己构造相似数据（正例）以及不相似数据（负例）<br><img src="/images/contrastivelearning-pic1.png" alt="png"><br>如图，对于某张图片，从可能的增强操作集合$T$中，随机抽取两种：$t_1-T$及$t_2-T$，分别作用在原始图像上形成两张经过增强的新图像$&lt;x_1, x_2&gt;$,两者互为正例。训练时，batch内其他任意图像，都可作为$x_1$或$x_2$的负例。这样，对比学习希望习得某个表示模型，它能够将图片映射到某个投影空间，并在这个空间内拉近正例的距离，推远负例距离。也就是说，迫使表示模型能够忽略表面因素，学习图像的内在一致结构信息，即学会某些类型的不变性，比如遮挡不变性、旋转不变性、颜色不变性等。SimCLR证明了，如果能够同时融合多种图像增强操作，增加对比学习模型任务难度，对于对比学习效果有明显提升作用。<br>有了正例和负例，接下来需要做的是：构造一个表示学习系统，通过它将训练数据投影到某个表示空间内，并采取一定的方法，使得正例距离能够比较近，负例距离比较远。在这个对比学习的指导原则下，我们来看SimCLR是如何构造表示学习系统的。<br><img src="/images/contrastivelearning-pic2.png" alt="png"><br>我们随机从无标训练数据中取N个构成一个Batch，对于Batch里的任意图像，根据上述方法构造正例，形成两个图像增强视图：Aug1和Aug2。Aug1 和Aug2各自包含N个增强数据，并分别经过上下两个分枝，对增强图像做非线性变换，这两个分枝就是SimCLR设计出的表示学习所需的投影函数，负责将图像数据投影到某个表示空间。<br>因为上下分枝是对称的，所以我们仅以增强视图Aug1所经过的上分枝来介绍投影过程。Aug1首先经过特征编码器Encoder（一般采用ResNet做为模型结构，这里以函数 $f_θ(x)$ 代表），经CNN转换成对应的特征表示 $h_i$ 。紧随其后，是另外一个非线性变换结构Projector（由[FC-&gt;BN-&gt;ReLU-&gt;FC]两层MLP构成，这里以函数 $g_θ(·)$ 代表），进一步将特征表示 $h_i$ 映射成另外一个空间里的向量 $z_i$ 。这样，增强图像经过 $g_θ(f_θ(x))$ 两次非线性变换，就将增强图像投影到了表示空间，下分枝的Aug2过程类似。这会引发一个问题：为什么这种投影操作，要做两次非线性变换，而不是直接在Encoder后，只经过一次变换即可呢？这个问题的答案，稍后我们会给出解释。<br>对于Batch内某张图像 $x$ 来说，在Aug1和Aug2里的对应的增强后图像分别是 $x_i$ 和 $x_j$ ，那么数据对 $&lt;x_i, x_j&gt;$ 互为正例，而 $x_i$ 和Aug1及Aug2里除 $x_j$ 之外的其它任意2N-2个图像都互为负例。在经过 $g_θ(f_θ(x))$ 变换后，增强图像被投影到表示空间。在表示空间内，我们希望正例距离较近，负例距离较远。如果希望达成这一点，一般通过定义合适的损失函数来实现。在介绍损失函数前，我们首先需要一个度量函数，以判断两个向量在投影空间里的距离远近，一般采用相似性函数来作为距离度量标准。具体而言，相似性计算函数采取对表示向量L2正则后的点积或者表示向量间的Cosine相似性：</p></blockquote><p>$$S(z_i, z_j) = z_i^Tz_j/(||z_i||_2||z_j||_2) \tag1$$</p><blockquote><p>SimCLR的损失函数采用InfoNCE Loss。  </p></blockquote><p>$$L_{bt} = \sum_i(1-C_{ii})^2 + \lambda \sum_i\sum_{j \neq i}C_{ij}^2  \tag2$$</p><blockquote><p>其中$&lt;z_i, z_i^+&gt;$表示两个正例相应的表示向量。函数分子部分鼓励正例相似度越高越好，分母鼓励负例之间相似度越低越好。<br><strong>上面介绍了SimCLR的关键做法，本身这个过程，其实是标准的预训练模式；利用海量的无标注图像数据，根据对比学习指导原则，学习出好的Encoder模型以及它对应产生的特征表示。所谓好的Encoder，就是说输入图像，它能学会并抽取出关键特征，这个过程跟Bert模型通过MLM自监督预训练其实目的相同，只是做法有差异。学好Encoder后，可以在解决下游具体任务的时候，用学到的参数初始化Encoder中的ResNet模型，用下游任务标注数据来Fine-tuning模型参数，期待预训练阶段学到的知识对下游任务有迁移作用。由此可见，SimCLR看着有很多构件，比如Encoder、Projector、图像增强、InfoNCE损失函数，其实我们最后要的，只是Encoder，而其它所有构件以及损失函数，只是用于训练出高质量Encoder的辅助结构。目前所有对比学习模型都是如此，这点还请注意</strong><br>至于为什么使用encoder和projector做两次投影变换，则是出自经验结果，实验证明，加上projector效果提升明显。<br>SimCLR论文中，对于Projector和Encoder的编码差异进行了对比实验，结论是：Encoder后的特征表示，会有更多包含图像增强信息在内的细节特征，而这些细节信息经过Projector后，很多被过滤掉了。虽然为何需要两次非线性变换，目前只有实验结果，并未有理论解释。可能是如下原因：一般的特征抽取器，在做特征提取的时候，底层偏向抽取通用的低层特征，往往与任务无关，通用性强；接近比如分类任务的高层网络结构，更倾向编码任务相关的高阶特征信息。想来，Encoder和Projector也应该如此，也就是说，在接近任务的高层网络，也就是Projector，会编码更多跟对比学习任务相关的信息，低层就是Encoder，会编码更多跟任务无关的通用细节信息。对于下游任务，这种对比学习训练任务相关的特征，可能会带来负面影响。如果映射网络只包含Encoder的话，那么特征表示里会有很多预训练任务相关特征，会影响下游任务效果；而加上Projector，等于增加了网络层深，这些任务相关特征就聚集在Projector，此时Encoder则不再包含预训练任务相关特征，只包含更通用的细节特征。这是为何需要两次映射过程。以上仅为一种可能原因，并不保证正确性。在此之后的对比学习模型，基本都采取了Encoder+Projector的两次映射结构，以及复合图像增强方法。  </p></blockquote><h4 id="对比学习到底在干什么"><a href="#对比学习到底在干什么" class="headerlink" title="对比学习到底在干什么"></a>对比学习到底在干什么</h4><blockquote><p>对比学习在做特征表示相似性计算时，要先对表示向量做L2正则，之后再做点积计算，或者直接采用Cosine相似性，为什么要这么做呢？现在很多研究表明，把特征表示 $g_θ(f_\Theta(x))$ 映射到单位超球面上，有很多好处。这里有两个关键，一个是单位长度，一个是超球面。首先，相比带有向量长度信息的点积，在去掉长度信息后的单位长度向量 $g_θ(f_θ(x))/{||g_θ(f_θ(x))||}^2$ 上操作，能增加深度学习模型的训练稳定性。另外，当表示向量 $g_θ(f_θ(x))$ 被映射到超球面上，如果模型的表示能力足够好，能够把相似的例子（比如带有相同类标号的数据）在超球面上聚集到较近区域，那么很容易使用线性分类器把某类和其它类区分开（参考上图）。在对比学习模型里，对学习到的表示向量 $g_θ(f_θ(x))$ 进行L2正则，或者采用Cosine相似性，就等价于将表示向量 $g_θ(f_θ(x))$ 投影到了单位超球面上进行相互比较。很多对比学习模型相关实验也证明了：对表示向量进行L2正则能提升模型效果。这是为何一般要对表示向量进行L2正则操作的原因。<br>那么，好的对比学习系统，应该具备怎样的潜在能力呢？论文“Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere”对这个问题进行了探讨。它提出了好的对比学习系统应该具备两个属性：Alignment和Uniformity（参考上图）。所谓“Alignment”，指的是相似的例子，也就是正例，映射到单位超球面后，应该有接近的特征，也即是说，在超球面上距离比较近；所谓“Uniformity”，指的是系统应该倾向在特征里保留尽可能多的信息，这等价于使得映射到单位超球面的特征，尽可能均匀地分布在球面上，分布得越均匀，意味着保留的信息越充分。乍一看不好理解“分布均匀和保留信息”两者之间的关联，其实道理很简单：分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分。<br><img src="/images/contrastivelearning-pic4.png" alt="png"><br>回顾infoNCE Loss,分子部分鼓励正例之间越近越好，这体现了Alignment属性；分母部分鼓励负例直接距离越远越好，这体现了Uniformity属性，使得负例的特征更均匀的分布在球面。<br>损失函数中的温度（tempture）参数（一帮设置为0.1或0.2）$\tau$易被忽视，实际上作用很大（在AAAI中也用到了这个超参），温度参数会将模型更新的重点，聚焦到有难度的负例，并对它们做相应的惩罚，难度越大，也即是与$x_i$距离越近，则分配到的惩罚越多。所谓惩罚，就是在模型优化过程中，将这些负例从 $x_i$ 身边推开，是一种斥力。也就是说，距离 $x_i$ 越近的负例，温度超参会赋予更多的排斥力，将它从 $x_i$ 推远。而如果温度超参数 $\tau$ 设置得越小，则InfoNCE分配惩罚项的范围越窄，更聚焦在距离 $x_i$ 比较近的较小范围内的负例里。同时，这些被覆盖到的负例，因为数量减少了，所以，每个负例，会承担更大的斥力。但是$\tau$并非越小越好，太小的温度参数有可能会将正例也造成惩罚。</p></blockquote><h4 id="更多对比学习模型"><a href="#更多对比学习模型" class="headerlink" title="更多对比学习模型"></a>更多对比学习模型</h4><blockquote><p>可以参考文章的<strong>基于负例的对比学习：Batch之外的<a href="https://zhuanlan.zhihu.com/p/367290573">部分</a>。</strong></p></blockquote><h4 id="问题与方向：向何处去"><a href="#问题与方向：向何处去" class="headerlink" title="问题与方向：向何处去"></a>问题与方向：向何处去</h4><blockquote><p>我们知道，自监督方法的一个潜在好处是；理论上随着无标注数据训练数据量的增大，那么自监督模型学习到的图像天然结构知识越丰富，应用在下游业务效果也应该越好。但是，从这一点上来看，目前的对比学习是有局限的，实验表明，当自监督数据量增加到非常大的时候，貌似并未能带来更多明显收益与效果。这说明目前的对比学习也好，自监督学习也好，仍然有很大的改进空间。<br>训练数据偏置（Bias）问题<br>目前大多数对比学习采用的自监督训练数据，一般会用ImageNet，但是用ImageNet做为对比学习训练数据，有个问题： ImageNet这种精挑细选出来的用于分类的数据，往往一个图片仅包含单个类别的实例，这也好理解，本来就是用来分类的，如果一张图片包含多个不同类的实体，那分类标签没法打。但是这带来的问题是：这种数据还是太干净了，如果我们在网上随意挑选图片，大多数都在描述一个场景，情况比较复杂，可能一张图片包含多个不同类别的实例。而目前的对比学习方法，对于处理此类一张图包含多类别实体的图片，效果并不好，这就是所谓的训练数据偏置问题。我个人认为，这个问题还是比较严重的，因为要想真正做到自监督，对训练数据要求就要放低，这样才能快速扩充更多数据用来训练。CASTing Your Model:Learning to Localize Improves Self-Supervised Representations文章探讨了这个问题并提出了一些解决办法。</p></blockquote><blockquote><p>更好构建正例的方法<br>正确地做图像增强，构建好的有难度的正例，对于对比学习是十分关键的。这样我们可以让表示学习系统，学到更多种类的图像不变性，增强表示学习模型的表达能力。虽然目前有很多图像增强方法，但是目前研究（参考：Demystifying Contrastive Self-Supervised Learning:Invariances, Augmentations and Dataset Biases）表明，与监督学习相比，对比学习模型主要学习到的，更多是一种图像遮挡不变性和颜色不变性，对于其它的常见不变形，比如视角不变性、照明不变性等，对比学习模型的效果要明显弱于监督学习。所以，如何构造正例，使得对比学习模型能学习更多种类的空间不变性，也是非常关键的。</p></blockquote><blockquote><p>像素级学习能力<br>从目前对比学习模型的运行机制看，因为是判别模型，目标相对容易，表示学习系统能学到细粒度的特征信息，但是对于像素级特征，应该是表达能力不足的。而图像处理具体的子领域众多，除了常见的分类任务，很多任务比如Object Segmentation等，都需要在像素级进行操作，所以如何改造现有对比学习模型，使得它能够更好地帮助像素级下游任务，也是比较重要的。论文Dense Contrastive Learning for Self-Supervised Visual Pre-Training针对这个问题，提出了改进方法。</p></blockquote><blockquote><p>除了上述几个问题，其实还有很多可以列在这里的，比如只使用正例的对比学习模型，从原理上讲，到底为何模型能够不坍塌？再比如，目前很多探索，集中研究对比学习中的Hard负例问题。我们从上文讲解可知：对于负例对比学习方法，之所以负例越多模型效果越好，其实本质上，是因为越多负例，会包含更多的Hard负例，而这些Hard负例对于模型贡献较大，而easy负例，其实没多少贡献。但是，我们又知道，温度超参本身其实是可以聚焦在Hard负例上的。那么，Hard负例应该研究什么具体问题？这个需要仔细考虑。再比如，目前不少研究在考虑融合有监督模型和对比学习，试图兼具两者的优点，这个有多大意义？……..诸如此类，很多问题与方向，不一而足，篇幅原因，到此为止。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日常学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>依存句法分析学习</title>
      <link href="/2021/07/30/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0/"/>
      <url>/2021/07/30/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="依存句法分析学习"><a href="#依存句法分析学习" class="headerlink" title="依存句法分析学习"></a>依存句法分析学习</h1><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><blockquote><p>由于项目需要，得分析如何解决中文句子里面量词和名词的搭配问题。之前考虑使用CSC、CGED任务中比较完善的模型去解。项目中已经用到CSC中效果比较好的BERT+GCN去解决，<strong>受到来自<a href="https://github.com/ACL2020SpellGCN/SpellGCN">SpellGCN</a>的启发</strong>，使用GCN替代传统的MLP，从而对字音字形相似具有更好的处理。但发现对这种指定的搭配错误没什么效果，于是准备先用依存句法分析去解决，如果效果也不如预期，在考虑针对此类任务扩充训练数据。</p></blockquote><h3 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h3><blockquote><p>考虑到要解决量词和名词的搭配问题，决定先从词性标注的角度出发，把一组量词-名词对找出来，但这样的问题在于量词名词间可能有很多修饰词，另一方面可能量词在名词后，如句子：“参与者可获得汽车一台。”<br>自己用规则写了半天，自以为很完备，但总有想不到的情况。</p></blockquote><blockquote><p>在师兄提醒下尝试使用依存句法分析去解决，个人使用了LTP、HanLP等工具，都有些不尽人意。最后考虑了<strong>百度开源的<a href="https://github.com/baidu/DDParser">DDParser</a>项目</strong>。基本流程如下：</p></blockquote><ul><li>先用百度的LAC分词工具进行分词，这里也需要用到分词后的词性。</li><li>封装分词后的结果作为dataset并以此构建dataloader</li><li>feed入ernie-lstm模型并获取各个词的head（关系对象）以及关系。  </li></ul><blockquote><p>DDParser项目参考的是2017年的<strong>ICLR<a href="https://arxiv.org/abs/1611.01734">《Deep Biaffine Attention for Neural Dependency Parsing》</a>一文</strong>，于是顺藤摸瓜，看看其依存句法分析的思路。  </p></blockquote><h3 id="论文解读"><a href="#论文解读" class="headerlink" title="论文解读"></a>论文解读</h3><blockquote><p>这篇论文是斯坦福专攻Dependency Parsing的博士生Dozat在ICLR 2017上的论文，拿到了graph-based方法中的最高分，改进版还拿到了CoNLL 2017 Shared Task的第一。基于图的依存句法分析需要解决两个问题：1、哪两个节点连依存弧；2、弧的标签是什么。NN方法使用分类器来解决这两个问题，记feature detector得到的特征向量为$r_i$。  </p></blockquote><h4 id="arc得分"><a href="#arc得分" class="headerlink" title="arc得分"></a>arc得分</h4><blockquote><p>这是一个不定类别的多分类问题。若句子中有n个单词，包含虚根ROOT在内一共d=n+1个token。对每个token来讲，都需要得到一个分数向量$s_i∈R^{d×1}$。所有token构成$R^{d×d}$的分数矩阵，最后根据arc-factored原理找MST(最小生成树)。<br>一般的MLP是固定类别的分类器，无法解决不定类别的分类问题。作者提出先将特征向量$r_i$做一个线性变换降维为k，即$R^{k×1}$。<br>$$h_i^{(arc_dep)} = MLP^{(arc_dep)}(r_i) \tag1$$<br>$$h_j^{(arc_head)} = MLP^{(arc_head)}(r_j) \tag2$$</p></blockquote><blockquote><p>其中MLP为dep和head专用，k通常维度更小，这样压缩维度的好处是可以去除一些冗余信息。<br>特征降维了之后，权值矩阵和偏置也必须做出调整。作者提出用两个矩阵连乘（两次仿射变换biaffine）输入向量:<br>$$s_i^{(arc)} = {H^{(arc_head)}U^{(1)}h_i^{(arc-dep)} + H^{(arc_head)}u^{(2)}} \tag3$$</p></blockquote><blockquote><p>其中矩阵H是d个token的特征经过MLP后的stack形式，上式维度变化为：<br>$$(d×k)(k×k)(k×1)+(d×k)(k×1)=(d×1) $$</p></blockquote><blockquote><p>结果是拿到了d个token的分数$R^{d×1}$，同时分类器又不需要维护多个不同大小的权值矩阵（只需一个$R^{k×k}$的矩阵和两个MLP），漂亮地实现了可变类别分类。<br>如果把偏置放到$U^{(1)}$里面去，同时进行d个token的运算，则上式的维度变化是:<br>$$(d×(k+1))((k+1)×k)(k×d)=(d×d)$$</p></blockquote><blockquote><p>作者称这3个式子为deep bilinear attention mechanism，因为不同于直接用RNN输出的feature （shallow），这里通过MLP二次encode了一下。叫attention的原因是，输入是所有时刻的RNN feature，输出是一个在各个时刻上归一化的向量。<br>整个网络架构如下:</p></blockquote><p><img src="/images/DEPModel.png" alt="png"> </p><blockquote><p>从下往上看，输入是word以及pos的embedding拼接，BiLSTM提取到特征$r_i$，经过两个不同的MLP分别得到$h^{(arc−dep)}_i$ 和 $h^{(arc−head)}_j$ ，d个这样的h stack起来分别构成了$H^{(arc−dep)}$和$H^{(arc−head)}$。其中由于bias的原因，$H^{(arc−dep)}$额外拼接了一个单位向量。经过一个中间矩阵$U^{(arc)}$仿射变换后，每个token以dep的身份与以head的身份的每个token进行一次点积，得到arc成立的分数矩阵$S^{(arc)}$。</p></blockquote><h4 id="arc标签"><a href="#arc标签" class="headerlink" title="arc标签"></a>arc标签</h4><blockquote><p>这是个固定类别的分类问题，作者认为必须同时考虑依存关系的先验概率，已知词语作为head或dep接受某种依存关系的后验概率。也就是下式:<br>$$S_i^{(label)}= {r^{T}_{(y_i)}U^{(1)}}+ (ry_i ⊕ r_i)^{T}U^{(2)} + b \tag4$$<br>第一项是同时已知i作为dep、$y_i$作为head情况下的后验概率，第二项是已知i或$y_i$是arc两端的后验概率，第三项偏置是什么都不知道时label的先验概率。<br>关于第二项的⊕的解读。拼接的话向量是有先后顺序的，应该用没有顺序的求和。这里看看两个向量（书写简便，就用仅含有一个元素的“向量”做例子）x1和x2的拼接乘上一个矩阵:</p></blockquote><p>$$\left(\begin{array}{c}a&amp;d\\<br>b&amp;e\\<br>c&amp;f\end{array} \right)<br>\left(\begin{array}{c}x_1\\<br>x_2 \end{array}\right) = \left(\begin{array}{c}ax_1+dx_2\\<br> bx_1+ex_2 \\<br> cx_1+fx_2\end{array}\right)$$</p><blockquote><p>可以看出，拼接后的向量与一个向量相乘的结果，是与拼接顺序无关的。第一项中含有一个高阶tensor，假设一共有m种label，上式第一项的维度变化为：<br>$$(1×k)(m×k×k)(k×1)=(m×1×1)$$</p></blockquote><h3 id="源码浅析"><a href="#源码浅析" class="headerlink" title="源码浅析"></a>源码浅析</h3><blockquote><p>项目源码的模型为ernie-lstm，使用ernie获取embedding，然后再用上述方法分别得到arc和relationship。</p></blockquote><h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><blockquote><p>利用DDParser，取出具有定中关系的、且包含量词（通过返回的词性判断量词）的词对。以名词为key在已有词典中做匹配，如果当前量词不在key对应的value list中，则为量词-名词搭配错误，并给出改正建议。</p></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><blockquote><p>方法较之我之前写的规则要简单，效果也有保证。在阅读论文过程中，作者用的几个仿射变换十分巧妙，我阅读时从要得到的结果反推中间的仿射会比较好理解，相信作者的思路也与之相似。最后就是DDParser介入了预训练模型（ERNIE），pretrained model的引入已经是一种趋势了。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 依存句法分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> PaperReading </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
